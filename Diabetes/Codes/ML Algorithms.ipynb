{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Gaussian NB:  0.9333333333333333\n",
      "F1 Score for Gaussian NB:  0.9327318776235065\n",
      "Kappa Score for Gaussian NB: 0.7896434456403605\n",
      "Precision for Gaussian NB: 0.9326480829748804\n",
      "Recall for Gaussian NB:  0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# GAUSSIAN NAIVE BAYES BEFORE FEATURE SELECTION\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "X = data[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = data['CLASS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% testing\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Gaussian NB: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Gaussian NB: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Gaussian NB:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Gaussian NB:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Gaussian NB: \", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Specs       Score\n",
      "0    AGE  323.975398\n",
      "9    BMI  283.742879\n",
      "3  HbA1c  242.873059\n",
      "8   VLDL  111.041655\n",
      "2     Cr   80.716030\n",
      "Accuracy for Gaussian NB:  0.9366666666666666\n",
      "F1 Score for Gaussian NB:  0.9362141471343924\n",
      "Kappa Score for Gaussian NB: 0.7998525229116191\n",
      "Precision for Gaussian NB: 0.9363384522734117\n",
      "Recall for Gaussian NB:  0.9366666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# GAUSSIAN NAIVE BAYES AFTER FEATURE SELECTION\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "X = data[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = data['CLASS']\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=5)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(5,'Score'))  #print 10 best features\n",
    "\n",
    "# Now you can select only those features from X which are in the top 10\n",
    "top_features = featureScores.nlargest(5,'Score')['Specs'].values\n",
    "X = X[top_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% testing\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Gaussian NB: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Gaussian NB: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Gaussian NB:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Gaussian NB:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Gaussian NB: \", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forests:  0.9766666666666667\n",
      "F1 Score for Random Forests:  0.9748732886984196\n",
      "Kappa Score for Random Forests: 0.9239102866045872\n",
      "Precision for Random Forests: 0.9737291947818263\n",
      "Recall for Random Forests:  0.9766666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Random Forests BEFORE FEATURE SELECTION\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "data = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "X = data[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = data['CLASS']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Random Forests: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Random Forests: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Random Forests:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Random Forests:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Random Forests: \", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Specs       Score\n",
      "0    AGE  323.975398\n",
      "9    BMI  283.742879\n",
      "3  HbA1c  242.873059\n",
      "8   VLDL  111.041655\n",
      "2     Cr   80.716030\n",
      "5     TG   28.204749\n",
      "Accuracy for Random Forests:  0.97\n",
      "F1 Score for Random Forests:  0.9681861471861472\n",
      "Kappa Score for Random Forests: 0.9006659063316287\n",
      "Precision for Random Forests: 0.9669076305220885\n",
      "Recall for Random Forests:  0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Random Forests AFTER FEATURE SELECTION\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "X = data[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = data['CLASS']\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=6)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(6,'Score'))  #print 10 best features\n",
    "\n",
    "# Now you can select only those features from X which are in the top 10\n",
    "top_features = featureScores.nlargest(6,'Score')['Specs'].values\n",
    "X = X[top_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Random Forests: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Random Forests: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Random Forests:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Random Forests:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Random Forests: \", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Gradient Boosting:  0.915\n",
      "F1 Score for Gradient Boosting:  0.9190275429331581\n",
      "Kappa Score for Gradient Boosting: 0.7177017602125539\n",
      "Precision for Gradient Boosting:  0.9281018518518519\n",
      "Recall for Gradient Boosting:  0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting BEFORE FEATURE SELECTION\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = data['CLASS']\n",
    "\n",
    "# Assuming X is your features and y is your target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a GradientBoostingClassifier object\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Gradient Boosting: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Gradient Boosting: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Gradient Boosting:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Gradient Boosting: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Gradient Boosting: \", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Specs       Score\n",
      "0    AGE  323.975398\n",
      "9    BMI  283.742879\n",
      "3  HbA1c  242.873059\n",
      "8   VLDL  111.041655\n",
      "Accuracy for Gradient Boosting:  0.91\n",
      "F1 Score for Gradient Boosting:  0.9173768148127586\n",
      "Kappa Score for Gradient Boosting: 0.7194950911640954\n",
      "Precision for Gradient Boosting:  0.9371721822209915\n",
      "Recall for Gradient Boosting:  0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting AFTER FEATURE SELECTION\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = data[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = data['CLASS']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=4)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(4,'Score'))  #print 10 best features\n",
    "\n",
    "# Now you can select only those features from X which are in the top 10\n",
    "top_features = featureScores.nlargest(4,'Score')['Specs'].values\n",
    "X = X[top_features]\n",
    "\n",
    "# Assuming X is your features and y is your target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a GradientBoostingClassifier object\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Gradient Boosting: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Gradient Boosting: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Gradient Boosting:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Gradient Boosting: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Gradient Boosting: \", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Logistic Regression:  0.9566666666666667\n",
      "F1 Score for Logistic Regression:  0.9543703703703704\n",
      "Kappa Score for Logistic Regression: 0.8134506840141587\n",
      "Precision for Logistic Regression:  0.9549404761904761\n",
      "Recall for Logistic Regression:  0.9566666666666667\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression BEFORE FEATURE SELECTION\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "# Assume that you are trying to predict a multi-class outcome variable 'y' based on some features 'X1', 'X2', 'X3'\n",
    "X = df[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = df['CLASS']\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create an instance of the Logistic Regression model\n",
    "logistic_regression= LogisticRegression(multi_class='multinomial',max_iter=35000)\n",
    "\n",
    "# Fit the model using the training data\n",
    "logistic_regression.fit(X_train,y_train)\n",
    "\n",
    "# Use the model to make predictions on the test data\n",
    "y_pred=logistic_regression.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Logistic Regression: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Logistic Regression: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Logistic Regression:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Logistic Regression: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Logistic Regression: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Specs       Score\n",
      "0    AGE  323.975398\n",
      "9    BMI  283.742879\n",
      "3  HbA1c  242.873059\n",
      "8   VLDL  111.041655\n",
      "2     Cr   80.716030\n",
      "5     TG   28.204749\n",
      "Accuracy for Logistic Regression:  0.9466666666666667\n",
      "F1 Score for Logistic Regression:  0.9346499994150083\n",
      "Kappa Score for Logistic Regression: 0.7498697238144867\n",
      "Precision for Logistic Regression:  0.9335132669983417\n",
      "Recall for Logistic Regression:  0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression AFTER FEATURE SELECTION\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "# Assume that you are trying to predict a multi-class outcome variable 'y' based on some features 'X1', 'X2', 'X3'\n",
    "X = df[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = df['CLASS']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=6)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(6,'Score'))  #print 10 best features\n",
    "\n",
    "# Now you can select only those features from X which are in the top 10\n",
    "top_features = featureScores.nlargest(6,'Score')['Specs'].values\n",
    "X = X[top_features]\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create an instance of the Logistic Regression model\n",
    "logistic_regression= LogisticRegression(multi_class='multinomial',max_iter=35000)\n",
    "\n",
    "# Fit the model using the training data\n",
    "logistic_regression.fit(X_train,y_train)\n",
    "\n",
    "# Use the model to make predictions on the test data\n",
    "y_pred=logistic_regression.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Logistic Regression: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Logistic Regression: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Logistic Regression:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Logistic Regression: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Logistic Regression: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Extra Trees:  0.9666666666666667\n",
      "F1 Score for Extra Trees:  0.9695156188616135\n",
      "Kappa Score for Extra Trees: 0.8586838758302322\n",
      "Precision for Extra Trees:  0.9730139011690736\n",
      "Recall for Extra Trees:  0.9666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Extra Trees BEFORE FEATURE SELECTION\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extra_trees = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "extra_trees.fit(X_train, y_train)\n",
    "\n",
    "y_pred = extra_trees.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Extra Trees: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Extra Trees: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Extra Trees:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Extra Trees: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Extra Trees: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Specs       Score\n",
      "0    AGE  323.975398\n",
      "9    BMI  283.742879\n",
      "3  HbA1c  242.873059\n",
      "8   VLDL  111.041655\n",
      "Accuracy for Extra Trees:  0.9766666666666667\n",
      "F1 Score for Extra Trees:  0.9801526649802514\n",
      "Kappa Score for Extra Trees: 0.9043802932337675\n",
      "Precision for Extra Trees:  0.9839481193255512\n",
      "Recall for Extra Trees:  0.9766666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Extra Trees AFTER FEATURE SELECTION\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "# Assume that you are trying to predict a multi-class outcome variable 'y' based on some features 'X1', 'X2', 'X3'\n",
    "X = df[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = df['CLASS']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=4)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(4,'Score'))  #print 10 best features\n",
    "\n",
    "# Now you can select only those features from X which are in the top 10\n",
    "top_features = featureScores.nlargest(4,'Score')['Specs'].values\n",
    "X = X[top_features]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extra_trees = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "extra_trees.fit(X_train, y_train)\n",
    "\n",
    "y_pred = extra_trees.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Extra Trees: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Extra Trees: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Extra Trees:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Extra Trees: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Extra Trees: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Bagging:  0.9866666666666667\n",
      "F1 Score for Bagging:  0.9900634624772556\n",
      "Kappa Score for Bagging: 0.9453601675621528\n",
      "Precision for Bagging:  0.9937777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for Bagging:  0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "# Bagging Trees BEFORE FEATURE SELECTION\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a decision tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Create a bagging classifier with the decision tree\n",
    "bagging = BaggingClassifier(base_estimator=tree, n_estimators=100, random_state=0)\n",
    "\n",
    "# Fit the model to your training data\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Bagging: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Bagging: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Bagging:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Bagging: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Bagging: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Specs       Score\n",
      "0    AGE  323.975398\n",
      "9    BMI  283.742879\n",
      "3  HbA1c  242.873059\n",
      "8   VLDL  111.041655\n",
      "Accuracy for Bagging:  0.9866666666666667\n",
      "F1 Score for Bagging:  0.9900634624772556\n",
      "Kappa Score for Bagging: 0.9453601675621528\n",
      "Precision for Bagging:  0.9937777777777778\n",
      "Recall for Bagging:  0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "#Bagging AFTER FEATURE SELECTION\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "# Assume that you are trying to predict a multi-class outcome variable 'y' based on some features 'X1', 'X2', 'X3'\n",
    "X = df[['AGE','Urea', 'Cr', 'HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y = df['CLASS']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=4)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(4,'Score'))  #print 10 best features\n",
    "\n",
    "# Now you can select only those features from X which are in the top 10\n",
    "top_features = featureScores.nlargest(4,'Score')['Specs'].values\n",
    "X = X[top_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Bagging Trees BEFORE FEATURE SELECTION\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a decision tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Create a bagging classifier with the decision tree\n",
    "bagging = BaggingClassifier(base_estimator=tree, n_estimators=100, random_state=0)\n",
    "\n",
    "# Fit the model to your training data\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Bagging: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Bagging: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Bagging:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Bagging: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Bagging: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for XGBoost:  0.995\n",
      "F1 Score for XGBoost:  0.995056727932834\n",
      "Kappa Score for XGBoost: 0.9814281734608599\n",
      "Precision for XGBoost:  0.9952500000000001\n",
      "Recall for XGBoost:  0.995\n"
     ]
    }
   ],
   "source": [
    "# XGBoost BEFORE FEATURE SELECTION\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Gender'])\n",
    "\n",
    "X = data.drop('CLASS', axis=1)\n",
    "y = data['CLASS']\n",
    "# Remove leading/trailing spaces\n",
    "y = y.str.strip()\n",
    "\n",
    "# Convert categorical variable to numerical\n",
    "y = y.map({'N': 0, 'P': 1, 'Y': 2})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Assuming y_test are your true classes and y_pred are your predicted classes\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for XGBoost: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for XGBoost: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for XGBoost:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for XGBoost: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for XGBoost: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19   0   0]\n",
      " [  0  11   0]\n",
      " [  1   0 169]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_test are your true labels and y_pred are the predicted labels\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature  Importance\n",
      "5       HbA1c    0.299015\n",
      "11        BMI    0.247056\n",
      "2         AGE    0.124813\n",
      "6        Chol    0.096337\n",
      "7          TG    0.054843\n",
      "10       VLDL    0.048931\n",
      "4          Cr    0.037668\n",
      "8         HDL    0.034904\n",
      "1   No_Pation    0.021386\n",
      "9         LDL    0.019407\n",
      "0          ID    0.011032\n",
      "12   Gender_0    0.002851\n",
      "3        Urea    0.001758\n",
      "13   Gender_1    0.000000\n",
      "Accuracy for XGBoost:  0.99\n",
      "F1 Score for XGBoost:  0.99\n",
      "Kappa Score for XGBoost: 0.9623281220568846\n",
      "Precision for XGBoost:  0.99\n",
      "Recall for XGBoost:  0.99\n"
     ]
    }
   ],
   "source": [
    "# XGBoost AFTER FEATURE SELECTION\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Gender'])\n",
    "\n",
    "X = data.drop('CLASS', axis=1)\n",
    "y = data['CLASS']\n",
    "# Remove leading/trailing spaces\n",
    "y = y.str.strip()\n",
    "\n",
    "# Convert categorical variable to numerical\n",
    "y = y.map({'N': 0, 'P': 1, 'Y': 2})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(importance_df)\n",
    "\n",
    "# Select the top 4 features\n",
    "top_4_features = importance_df['Feature'].iloc[:4]\n",
    "\n",
    "# Select only the top 4 features from your train and test data\n",
    "X_train_selected = X_train[top_4_features]\n",
    "X_test_selected = X_test[top_4_features]\n",
    "\n",
    "# Train and evaluate a new model on the selected features\n",
    "model_selected = xgb.XGBClassifier()\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = model_selected.predict(X_test_selected)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for XGBoost: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for XGBoost: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for XGBoost:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for XGBoost: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for XGBoost: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree: 0.97\n",
      "Precision of Decision Tree: 0.97\n",
      "F1-Score of Decision Tree: 0.97\n",
      "Kappa Index of Decision Tree: 0.88\n",
      "Recall of Decision Tree: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\revna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18100\\735304058.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precision of Decision Tree:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F1-Score of Decision Tree:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Kappa Index of Decision Tree:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcohen_kappa_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Recall of Decision Tree:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Log loss of Decision Tree:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m                         \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                     )\n\u001b[0;32m    213\u001b[0m                 ):\n\u001b[0;32m    214\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;31m# the function to delegate validation to the estimator, but we replace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m                 \u001b[1;31m# the name of the estimator by the name of the function in the error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[0;32m   2840\u001b[0m     >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n\u001b[0;32m   2841\u001b[0m     ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n\u001b[0;32m   2842\u001b[0m     \u001b[1;36m0.21616\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2843\u001b[0m     \"\"\"\n\u001b[1;32m-> 2844\u001b[1;33m     y_pred = check_array(\n\u001b[0m\u001b[0;32m   2845\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2846\u001b[0m     )\n\u001b[0;32m   2847\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    912\u001b[0m                         )\n\u001b[0;32m    913\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m                 raise ValueError(\n\u001b[0;32m    918\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2082\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m         if (\n\u001b[0;32m   2086\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2087\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Y'"
     ]
    }
   ],
   "source": [
    "#Decision Trees \n",
    "\n",
    "#loading the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "df=pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "x=df[['AGE','Urea','Cr','HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y=df[['CLASS']]\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.7,random_state=100)\n",
    "\n",
    "clf=DecisionTreeClassifier() \n",
    "clf=clf.fit(x_train, y_train) \n",
    "y_pred=clf.predict(x_test) \n",
    "\n",
    "print(\"Accuracy of Decision Tree:\",round(metrics.accuracy_score(y_test, y_pred),2))\n",
    "print(\"Precision of Decision Tree:\",round(metrics.precision_score(y_test,y_pred,average='weighted'),2))\n",
    "print(\"F1-Score of Decision Tree:\",round(metrics.f1_score(y_test,y_pred,average='weighted'),2))\n",
    "print(\"Kappa Index of Decision Tree:\",round(metrics.cohen_kappa_score(y_test,y_pred),2))\n",
    "print(\"Recall of Decision Tree:\",round(metrics.recall_score(y_pred,y_test,average='weighted'),2))\n",
    "print(\"Log loss of Decision Tree:\",round(metrics.log_loss(y_pred,y_test),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "df=pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "x=df[['AGE','Urea','Cr','HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y=df[['CLASS']]\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.6,random_state=100)\n",
    "\n",
    "#code to create a support vector machine\n",
    "clf=svm.SVC(kernel='linear') #creating an svm classifier (a linear kernel)\n",
    "clf.fit(x_train,y_train.values.ravel()) #training the classifier\n",
    "y_pred=clf.predict(x_test) #predicting the response for the chosen dataset\n",
    "\n",
    "print(\"Accuracy of Support Vector Machine:\",round(metrics.accuracy_score(y_test, y_pred),2))\n",
    "print(\"Precision of Support Vector Machine:\",round(metrics.precision_score(y_test,y_pred,average='weighted'),2))\n",
    "print(\"F1-Score of Support Vector Machine:\",round(metrics.f1_score(y_test,y_pred,average='weighted'),2))\n",
    "print(\"Kappa Index of Support Vector Machine:\",round(metrics.cohen_kappa_score(y_test,y_pred),2))\n",
    "print(\"Recall of Support Vector Machine:\",round(metrics.recall_score(y_pred,y_test,average='weighted'),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pandas as pd\n",
    "per = Perceptron()\n",
    "\n",
    "df=pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "x=df[['AGE','Urea','Cr','HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y=df[['CLASS']]\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.6,random_state=100)\n",
    "\n",
    "# Fit the model to your training data\n",
    "per.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = per.predict(x_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Perceptron: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Perceptron: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Perceptron:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Perceptron: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred, average='weighted') # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Perceptron: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('C:\\DiabetesData\\Diabetes_Dataset.csv')\n",
    "\n",
    "X=df[['AGE','Urea','Cr','HbA1c','Chol','TG','HDL','LDL','VLDL','BMI']]\n",
    "y=df[['CLASS']]\n",
    "\n",
    "# Split your dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a perceptron model\n",
    "model = Perceptron()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy for Perceptron: \", accuracy)\n",
    "\n",
    "# F1 Score\n",
    "f1_score = metrics.f1_score(y_test, y_pred) # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"F1 Score for Perceptron: \", f1_score)\n",
    "\n",
    "# Kappa Score\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Kappa Score for Perceptron:\", kappa_score)\n",
    "\n",
    "# Precision\n",
    "precision = metrics.precision_score(y_test, y_pred) # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Precision for Perceptron: \", precision)\n",
    "\n",
    "# Recall\n",
    "recall = metrics.recall_score(y_test, y_pred) # you can change the average parameter to 'micro', 'macro', 'weighted', depending on your problem\n",
    "print(\"Recall for Perceptron: \", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
